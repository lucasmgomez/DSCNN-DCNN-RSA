{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr3Q-rpQ0Wwq"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YGgE3ULgRRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c53dcd-e4a1-4acb-abf7-b7be9fd9ff3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nRZHATcgV5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c720b8cd-be94-4fbd-fcc2-408c3d901f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for SpykeTorch-miladmozafari (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/miladmozafari/SpykeTorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LSZQhxAgQgh"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio\n",
        "import os\n",
        "import ast\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.nn.parameter import Parameter\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from SpykeTorch import snn\n",
        "from SpykeTorch import functional as sf\n",
        "from SpykeTorch import visualization as vis\n",
        "from SpykeTorch import utils\n",
        "from torchvision import transforms\n",
        "\n",
        "dir = '/content/drive/MyDrive/Thesis/'\n",
        "cifar_dir = '/content/drive/MyDrive/Thesis/Data/extracted'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwt87YYixKdq"
      },
      "source": [
        "# Open"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsS5KifRxIqD"
      },
      "outputs": [],
      "source": [
        "import SpykeTorch.utils as utils\n",
        "import SpykeTorch.functional as sf\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def time_dim(input):\n",
        "    return input.unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "     time_dim,\n",
        "     sf.pointwise_inhibition,\n",
        "     utils.Intensity2Latency(number_of_spike_bins = 15, to_spike = True)])\n",
        "\n",
        "# kernels = [\tutils.GaborKernel(window_size = 3, orientation = 45+22.5),\n",
        "#             utils.GaborKernel(3, 90+22.5),\n",
        "#             utils.GaborKernel(3, 135+22.5),\n",
        "#             utils.GaborKernel(3, 180+22.5)]\n",
        "# filter = utils.Filter(kernels, use_abs = True)\n",
        "\n",
        "# def time_dim(input):\n",
        "#     return input.unsqueeze(0)\n",
        "\n",
        "# transform = transforms.Compose(\n",
        "#     [transforms.Grayscale(),\n",
        "#     transforms.ToTensor(),\n",
        "#     time_dim,\n",
        "#     filter,\n",
        "#     sf.pointwise_inhibition,\n",
        "#     utils.Intensity2Latency(number_of_spike_bins = 15, to_spike = True)])\n",
        "\n",
        "\n",
        "\n",
        "# class Transform_Caltech:\n",
        "#     def __init__(self,\n",
        "#                     pooling_size,\n",
        "#                     pooling_stride,\n",
        "#                     lateral_inhibition = None,\n",
        "#                     timesteps = 15,\n",
        "#                     feature_wise_inhibition=True):\n",
        "        \n",
        "#         self.transform = transforms.Compose((\n",
        "#             transforms.Grayscale(),\n",
        "#             transforms.ToTensor()))\n",
        "        \n",
        "#         self.kernels = [utils.GaborKernel(5, 45+22.5),\n",
        "#                         utils.GaborKernel(5, 90+22.5),\n",
        "#                         utils.GaborKernel(5, 135+22.5),\n",
        "#                         utils.GaborKernel(5, 180+22.5)]\n",
        "        \n",
        "#         self.filter = utils.Filter(self.kernels, use_abs = True)\n",
        "#         self.pooling_size = pooling_size\n",
        "#         self.pooling_stride = pooling_stride\n",
        "#         self.lateral_inhibition = lateral_inhibition\n",
        "#         self.in2lan = utils.Intensity2Latency(timesteps)\n",
        "#         self.feature_wise_inhibition = feature_wise_inhibition\n",
        "#     def __call__(self, image):\n",
        "#         image = self.transform(image)\n",
        "#         image.unsqueeze_(0)\n",
        "#         image = self.filter(image)\n",
        "#         image = sf.pooling(image, self.pooling_size, self.pooling_stride, padding=self.pooling_size//2)\n",
        "#         if self.lateral_inhibition is not None:\n",
        "#             image = self.lateral_inhibition(image)\n",
        "#         temporal_image = self.in2lan(image)\n",
        "#         temporal_image = sf.pointwise_inhibition(temporal_image)\n",
        "#         return temporal_image.sign().byte()\n",
        "\n",
        "# lateral_inhibition = utils.LateralIntencityInhibition([0.15, 0.12, 0.1, 0.07, 0.05])\n",
        "# inp = Transform_Caltech(7, 6, lateral_inhibition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM8w-93MhkMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a60a21-e494-46fd-eafb-964b40c314c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "CIFAR_train = utils.CacheDataset(torchvision.datasets.CIFAR10(root=cifar_dir, train=True, download=True, transform = transform))\n",
        "CIFAR_test = utils.CacheDataset(torchvision.datasets.CIFAR10(root=cifar_dir, train=False, download=True, transform = transform))\n",
        "CIFAR_loader = DataLoader(CIFAR_train, batch_size=1000, shuffle=False)\n",
        "CIFAR_testLoader = DataLoader(CIFAR_test, batch_size=len(CIFAR_test), shuffle=False)\n",
        "\n",
        "images, labels = next(CIFAR_loader.__iter__())  \n",
        "\n",
        "images[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lopQC2dgzOZ7"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUeb7Bm9xnGj"
      },
      "outputs": [],
      "source": [
        "class CSNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CSNN, self).__init__()\n",
        "\n",
        "        self.n_layers = 4\n",
        "\n",
        "        # Pooling Layer\n",
        "        self.pool = snn.Pooling(kernel_size = 3, stride = 1).cuda()\n",
        "\n",
        "        # Conv Layers\n",
        "        self.conv1 = snn.Convolution(in_channels=3, out_channels=64, kernel_size=4).cuda()\n",
        "        self.conv2 = snn.Convolution(in_channels=64, out_channels=128, kernel_size=3).cuda()\n",
        "        self.conv3 = snn.Convolution(in_channels=128, out_channels=256, kernel_size=3).cuda()\n",
        "        self.conv4 = snn.Convolution(in_channels=256, out_channels=200, kernel_size=11).cuda()\n",
        "\n",
        "        # STDP Functions\n",
        "        self.stdp1 = snn.STDP(conv_layer = self.conv1, learning_rate = (0.1, -0.005)).cuda()  #0.1, -0.005\n",
        "        self.stdp2 = snn.STDP(conv_layer = self.conv2, learning_rate = (0.07, -0.005)).cuda() #0.09, -0.005\n",
        "        self.stdp3 = snn.STDP(conv_layer = self.conv3, learning_rate = (0.05, -0.005)).cuda()  #0.08 -0.005\n",
        "        self.stdp4 = snn.STDP(self.conv4, (0.5, -0.05), False, 0.2).cuda() #(0.007, -0.01)\n",
        "        self.anti_stdp = snn.STDP(self.conv4, (-0.0002, 0.075), False, 0.2).cuda() # (-0.002, 0.0001)\n",
        "\n",
        "        # Spike Counts\n",
        "        self.spk_cnt1 = 0\n",
        "        self.spk_cnt2 = 0\n",
        "        self.spk_cnt3 = 0\n",
        "\n",
        "        # Max learning rate\n",
        "        self.max_ap = Parameter(torch.Tensor([0.3]))\n",
        "\n",
        "        # Feature to classes map\n",
        "        self.feature2class = []\n",
        "        for i in range(10):\n",
        "            self.feature2class.extend([i]*20)\n",
        "\n",
        "    def forward(self, x, t, layer):\n",
        "        if self.training:\n",
        "            # x = sf.pad(x.float(), (2,2,2,2))\n",
        "            if layer >= 1:\n",
        "                x = self.pool(x)\n",
        "                p = self.conv1(x)\n",
        "                o, p = sf.fire(p, 10, return_thresholded_potentials=True)\n",
        "                if layer == 1:\n",
        "                    self.spk_cnt1 += 1\n",
        "                    if self.spk_cnt1 >= 500:\n",
        "                        self.spk_cnt1 = 0\n",
        "                        ap = torch.tensor(self.stdp1.learning_rate[0][0].item(), device=self.stdp1.learning_rate[0][0].device) * 20\n",
        "                        ap = torch.min(ap, self.max_ap)\n",
        "                        an = ap * -0.75\n",
        "                        # self.stdp1.update_all_learning_rate(ap.item(), an.item())\n",
        "                    p = sf.pointwise_inhibition(p)\n",
        "                    o = p.sign()\n",
        "                    winners = sf.get_k_winners(p, kwta=5, inhibition_radius=0, spikes=o)\n",
        "\n",
        "            if layer >= 2: \n",
        "                # x = sf.pad(sf.pooling(o, 2, 2), (1,1,1,1))\n",
        "                x = o\n",
        "                p = self.conv2(o)\n",
        "                o, p = sf.fire(p, 12, return_thresholded_potentials=True)\n",
        "                if layer == 2:\n",
        "                    self.spk_cnt2 += 1\n",
        "                    if self.spk_cnt2 >= 500:\n",
        "                        self.spk_cnt2 = 0\n",
        "                        ap = torch.tensor(self.stdp2.learning_rate[0][0].item(), device=self.stdp2.learning_rate[0][0].device) * 2\n",
        "                        ap = torch.min(ap, self.max_ap)\n",
        "                        an = ap * -0.75\n",
        "                        # self.stdp2.update_all_learning_rate(ap.item(), an.item())\n",
        "                    p = sf.pointwise_inhibition(p)\n",
        "                    o = p.sign()\n",
        "                    winners = sf.get_k_winners(p, kwta=3, inhibition_radius=0, spikes=o)\n",
        "            if layer >= 3:\n",
        "                # x = sf.pad(sf.pooling(o, 2, 2), (2,2,2,2))\n",
        "                # x = self.pool(o)\n",
        "                x = o\n",
        "                p = self.conv3(x)\n",
        "                o, p = sf.fire(p, 5, return_thresholded_potentials=True)\n",
        "                if layer == 3:\n",
        "                    self.spk_cnt3 += 1\n",
        "                    if self.spk_cnt3 >= 500:\n",
        "                        self.spk_cnt3 = 0\n",
        "                        ap = torch.tensor(self.stdp3.learning_rate[0][0].item(), device=self.stdp3.learning_rate[0][0].device) * 2\n",
        "                        ap = torch.min(ap, self.max_ap)\n",
        "                        an = ap * -0.75\n",
        "                        # self.stdp3.update_all_learning_rate(ap.item(), an.item())\n",
        "                    p = sf.pointwise_inhibition(p)\n",
        "                    o = p.sign()\n",
        "                    winners = sf.get_k_winners(p, kwta=2, inhibition_radius=0, spikes=o)\n",
        "                    \n",
        "            if layer == 4:\n",
        "                x = o\n",
        "                p = self.conv4(x)\n",
        "                o = sf.fire(p)\n",
        "                winners = sf.get_k_winners(p, kwta=1, inhibition_radius=0, spikes=o)\n",
        "                if len(winners) != 0:\n",
        "                    if self.feature2class[winners[0][0]] == t:\n",
        "                        self.stdp(layer, x, p, o, winners)\n",
        "                        return winners\n",
        "                    elif layer == self.n_layers:\n",
        "                        self.anti_stdp(x, p, o, winners)\n",
        "                        return winners\n",
        "                return winners\n",
        "\n",
        "            # print(x.shape, o.shape, p.shape, len(winners))\n",
        "            self.stdp(layer, x, p, o, winners)\n",
        "            return winners\n",
        "\n",
        "        else:\n",
        "            x = self.pool(x.cuda())\n",
        "            # x = sf.pad(x.float(), (2,2,2,2), 0)\n",
        "            p = self.conv1(x)\n",
        "            o, p = sf.fire(p, 10, return_thresholded_potentials=True)\n",
        "            x = o\n",
        "            # x = sf.pad(sf.pooling(o, 2, 2), (1,1,1,1))\n",
        "            p = self.conv2(x)\n",
        "            o, p = sf.fire(p, 5, return_thresholded_potentials=True)\n",
        "            # x = self.pool(o)\n",
        "            # x = sf.pad(sf.pooling(o, 2, 2), (2,2,2,2))\n",
        "            x = o\n",
        "            p = self.conv3(x)\n",
        "            o, p = sf.fire(p, 2, return_thresholded_potentials=True)\n",
        "            p = self.conv4(o)\n",
        "            o = sf.fire(p)\n",
        "            winners = sf.get_k_winners(p, kwta=1, inhibition_radius=0, spikes=o)\n",
        "            return winners\n",
        "\n",
        "    def reset(self):\n",
        "        self.conv.reset_weight    \n",
        "\n",
        "    def stdp(self, layer, x, p, o, winners):\n",
        "        if layer == 1:\n",
        "            self.stdp1(x, p, o, winners)\n",
        "        elif layer == 2:\n",
        "            self.stdp2(x, p, o, winners)\n",
        "        elif layer == 3:\n",
        "            self.stdp3(x, p, o, winners)\n",
        "        else:\n",
        "            self.stdp4(x, p, o, winners)\n",
        "\n",
        "\n",
        "    def update_learning_rates(self, stdp_ap, stdp_an, anti_stdp_ap, anti_stdp_an):\n",
        "        self.stdp4.update_all_learning_rate(stdp_ap, stdp_an)\n",
        "        self.anti_stdp.update_all_learning_rate(anti_stdp_an, anti_stdp_ap)\n",
        "\n",
        "      \n",
        "def train(net, train_loader, layer, epoch=20):\n",
        "    logs = []\n",
        "    net.train()\n",
        "\n",
        "    for iter in range(epoch):\n",
        "        print('\\rIteration:', iter, end=\"\")\n",
        "        for data,targets in train_loader:\n",
        "            log = {'silent':0, 'total':0}\n",
        "            for x,t in zip(data, targets):\n",
        "                winners = net(x.cuda(), t.cuda(), layer)\n",
        "                log['total'] += 1\n",
        "                if len(winners) == 0:\n",
        "                    log['silent'] += 1\n",
        "        if (iter % 5 == 1 or iter == epoch - 1):\n",
        "            print(log)\n",
        "            logs.append(log)\n",
        "    return logs\n",
        "    print()\n",
        "    print(\"Unsupervised Learning is Done.\")\n",
        "\n",
        "\n",
        "def train_rl(net, train_loader, epoch=20):\n",
        "    logs = []\n",
        "    net.train()\n",
        "    # initial adaptive learning rates\n",
        "    apr = net.stdp4.learning_rate[0][0].item()\n",
        "    anr = net.stdp4.learning_rate[0][1].item()\n",
        "    app = net.anti_stdp.learning_rate[0][1].item()\n",
        "    anp = net.anti_stdp.learning_rate[0][0].item()\n",
        "\n",
        "    adaptive_min = 0\n",
        "    adaptive_int = 1\n",
        "    apr_adapt = ((1.0 - 1.0 / 5) * adaptive_int + adaptive_min) * apr\n",
        "    anr_adapt = ((1.0 - 1.0 / 5) * adaptive_int + adaptive_min) * anr\n",
        "    app_adapt = ((1.0 / 5) * adaptive_int + adaptive_min) * app\n",
        "    anp_adapt = ((1.0 / 5) * adaptive_int + adaptive_min) * anp\n",
        "\n",
        "    best_test = 1.0\n",
        "\n",
        "    for iter in range(epoch):\n",
        "        \n",
        "        print('\\rIteration:', iter, end=\"\")\n",
        "        for data,targets in train_loader:\n",
        "            log = {'silent':0, 'error':0,'total':0}\n",
        "            for x,t in zip(data, targets):\n",
        "                log['total'] += 1\n",
        "                winners = net(x.cuda(), t.cuda(), 4)\n",
        "                if len(winners) != 0:\n",
        "                    if net.feature2class[winners[0][0]] != t:\n",
        "                        log['error'] += 1\n",
        "                else:\n",
        "                    log['silent'] += 1 \n",
        "\n",
        "                correct = log['total']-log['error']-log['silent']\n",
        "\n",
        "                apr_adapt = apr * (log['error'] * adaptive_int + adaptive_min)\n",
        "                anr_adapt = anr * (log['error'] * adaptive_int + adaptive_min)\n",
        "                app_adapt = app * (correct * adaptive_int + adaptive_min)\n",
        "                anp_adapt = anp * (correct * adaptive_int + adaptive_min)\n",
        "                net.update_learning_rates(apr_adapt, anr_adapt, app_adapt, anp_adapt)\n",
        "\n",
        "            err = test(net, 4, CIFAR_testLoader)\n",
        "            if err < best_test:\n",
        "                best_test = err\n",
        "                print('NEW BEST: ' + str(best_test))\n",
        "                torch.save(net.state_dict(), l4_dir)\n",
        "            if err < 0.5:\n",
        "                break\n",
        "                \n",
        "        if (iter % 5 == 1 or iter == epoch - 1):\n",
        "            print(best_test)\n",
        "            print('epoch ' + str(epoch))\n",
        "            print(log)\n",
        "\n",
        "    print()\n",
        "    print(\"Reinforcement Learning is Done.\")\n",
        "\n",
        "def test(net, layer, test_loader):\n",
        "    net.eval()\n",
        "    error = 0\n",
        "    silent = 0\n",
        "    total = 0\n",
        "    for data,targets in test_loader:\n",
        "        for x,t in zip(data, targets):\n",
        "            winners = net(x.cuda(), t.cuda(), layer)\n",
        "            total += 1\n",
        "            if len(winners) != 0:\n",
        "                if net.feature2class[winners[0][0]] != t:\n",
        "                    error += 1\n",
        "            else:\n",
        "                silent += 1  \n",
        "    print(\"         Error:\", error/total)\n",
        "    print(\"Silent Samples:\", silent/total)\n",
        "    return error/total\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EQCboR_0fmi"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9jHyuDo7z47",
        "outputId": "0a5938ad-8008-4d49-eb71-37a4f6c654d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the first layer\n"
          ]
        }
      ],
      "source": [
        "csnn = CSNN().cuda()\n",
        "\n",
        "force_train = [0,0,0,0]\n",
        "\n",
        "# Training The First Layer'\n",
        "l1_dir = '/content/drive/MyDrive/Thesis/checkpoints/saved_l1.net'\n",
        "print(\"Training the first layer\")\n",
        "if force_train[0]:\n",
        "    train(csnn, CIFAR_loader, 1, 3)\n",
        "    torch.save(csnn.state_dict(), l1_dir)\n",
        "elif os.path.isfile(l1_dir):\n",
        "    csnn.load_state_dict(torch.load(l1_dir))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBhN90I4skzY",
        "outputId": "03dfcd8c-8733-4ec6-b57a-62ec0b697d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the second layer\n",
            "Training the third layer\n"
          ]
        }
      ],
      "source": [
        "# Training The Second Layer\n",
        "l2_dir = '/content/drive/MyDrive/Thesis/checkpoints/saved_l2.net'\n",
        "print(\"Training the second layer\")\n",
        "if force_train[1]:\n",
        "    train(csnn, CIFAR_loader, 2, 5)\n",
        "    torch.save(csnn.state_dict(), l2_dir)\n",
        "elif os.path.isfile(l2_dir):\n",
        "    csnn.load_state_dict(torch.load(l2_dir))\n",
        "\n",
        "csnn = CSNN().cuda()\n",
        "\n",
        "# Training The Third Layer\n",
        "l3_dir = '/content/drive/MyDrive/Thesis/checkpoints/saved_l3.net'\n",
        "print(\"Training the third layer\")\n",
        "if force_train[2]:\n",
        "    train(csnn, CIFAR_loader, 3, 5)\n",
        "    torch.save(csnn.state_dict(), l3_dir)\n",
        "elif os.path.isfile(l3_dir):\n",
        "    csnn.load_state_dict(torch.load(l3_dir))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bqiuOlosl36",
        "outputId": "e695b57b-fcb6-474a-d6d5-9e72c0e1f0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the fourth layer\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Training The Fourth Layer\n",
        "l4_dir = '/content/drive/MyDrive/Thesis/checkpoints/saved_l4.net'\n",
        "print(\"Training the fourth layer\")\n",
        "if force_train[3]:\n",
        "    train_rl(csnn, CIFAR_loader, 15)\n",
        "elif os.path.isfile(l4_dir):\n",
        "    csnn.load_state_dict(torch.load(l4_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0tkZfuVKHZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e63a17-df2f-4e0f-a0c7-53cfcc19c4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Error: 0.9\n",
            "Silent Samples: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "test(csnn, 4, CIFAR_testLoader) #0.35485"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Qgj6RSKS43"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wr3Q-rpQ0Wwq",
        "Hwt87YYixKdq"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "file_extension": ".py",
    "gpuClass": "standard",
    "interpreter": {
      "hash": "1f47fff126ce7d03424fc014a0fe52872e91ae166e540ba7367177e2e1478c83"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6  ('spyketorchproject': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}